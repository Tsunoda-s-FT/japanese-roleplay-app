import { useState, useCallback, useRef, useEffect } from 'react';
import { ScenarioKey, roleplayScenarios } from '@/lib/agents/roleplayAgent';

export interface SessionState {
  isConnecting: boolean;
  isConnected: boolean;
  error: string | null;
  isMuted: boolean;
  scenario: ScenarioKey | null;
  history: Array<{
    role: 'user' | 'assistant';
    content: string;
    timestamp: Date;
  }>;
}

// ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šAPIã‚­ãƒ¼ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼‰
export function useRealtimeDirectAPI() {
  const [state, setState] = useState<SessionState>({
    isConnecting: false,
    isConnected: false,
    error: null,
    isMuted: false,
    scenario: null,
    history: [],
  });

  const pcRef = useRef<RTCPeerConnection | null>(null);
  const dcRef = useRef<RTCDataChannel | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  const connect = useCallback(async (scenario: ScenarioKey) => {
    console.log('ğŸ”µ === Direct API Connection Test ===');
    setState(prev => ({ ...prev, isConnecting: true, error: null, scenario }));

    try {
      // APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆé–‹ç™ºç”¨ï¼‰
      const apiKey = process.env.NEXT_PUBLIC_OPENAI_API_KEY;
      if (!apiKey) {
        throw new Error('NEXT_PUBLIC_OPENAI_API_KEY is not set in .env.local');
      }
      
      console.log('ğŸ”‘ Using direct API key (first 20 chars):', apiKey.substring(0, 20) + '...');

      // RTCPeerConnectionä½œæˆ
      console.log('ğŸ”— Creating RTCPeerConnection...');
      const pc = new RTCPeerConnection({
        iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
      });
      pcRef.current = pc;

      // ICEå€™è£œåé›†ã®å®Œäº†ã‚’å¾…ã¤ãŸã‚ã®Promise
      const iceGatheringComplete = new Promise<void>((resolve) => {
        pc.onicegatheringstatechange = () => {
          console.log('ğŸ§Š ICE gathering state:', pc.iceGatheringState);
          if (pc.iceGatheringState === 'complete') {
            resolve();
          }
        };
      });

      // éŸ³å£°å‡ºåŠ›è¨­å®š
      const audioEl = document.createElement('audio');
      audioEl.autoplay = true;
      audioEl.style.display = 'none';
      document.body.appendChild(audioEl);
      audioRef.current = audioEl;

      pc.ontrack = (event) => {
        console.log('ğŸ”Š Received audio track:', event.track.kind);
        if (event.track.kind === 'audio') {
          audioEl.srcObject = event.streams[0];
          // éŸ³é‡ã‚’ç¢ºèª
          audioEl.volume = 1.0;
          audioEl.play().then(() => {
            console.log('ğŸ”Š Audio playback started');
          }).catch(err => {
            console.error('âŒ Audio playback error:', err);
          });
        }
      };

      // ãƒã‚¤ã‚¯è¨­å®š
      console.log('ğŸ¤ Requesting microphone access...');
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        } 
      });
      streamRef.current = stream;
      
      stream.getTracks().forEach(track => {
        pc.addTrack(track, stream);
      });
      console.log('âœ… Microphone added');

      // ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒãƒ«ä½œæˆï¼ˆåå‰ã¯æ­£ç¢ºã« 'oai-events'ï¼‰
      const dc = pc.createDataChannel('oai-events', {
        ordered: true,
      });
      dcRef.current = dc;

      // ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒãƒ«ã®ã‚¤ãƒ™ãƒ³ãƒˆè¨­å®š
      dc.onopen = () => {
        console.log('âœ… Data channel opened');
        setState(prev => ({ 
          ...prev, 
          isConnecting: false, 
          isConnected: true 
        }));

        // ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚’é€ä¿¡
        const sessionConfig = {
          type: 'session.update',
          session: {
            modalities: ['text', 'audio'],
            instructions: roleplayScenarios[scenario].instructions,
            voice: 'alloy',
            input_audio_format: 'pcm16',
            output_audio_format: 'pcm16',
            input_audio_transcription: { 
              model: 'whisper-1' 
            },
            turn_detection: { 
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 200,
            },
          },
        };
        console.log('ğŸ“¤ Sending session config:', sessionConfig);
        dc.send(JSON.stringify(sessionConfig));
        
        // åˆå›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦ä¼šè©±ã‚’é–‹å§‹
        setTimeout(() => {
          console.log('ğŸ‘‹ Sending initial greeting...');
          const initialMessage = {
            type: 'conversation.item.create',
            item: {
              type: 'message',
              role: 'user',
              content: [{
                type: 'input_text',
                text: 'ã“ã‚“ã«ã¡ã¯'
              }]
            }
          };
          dc.send(JSON.stringify(initialMessage));
          
          // å¿œç­”ã‚’ç”Ÿæˆ
          dc.send(JSON.stringify({ type: 'response.create' }));
        }, 1000);
      };

      dc.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log('ğŸ“¥ Received:', data.type, data);

          // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã”ã¨ã®å‡¦ç†
          switch (data.type) {
            case 'session.created':
              console.log('âœ… Session created');
              break;
              
            case 'session.updated':
              console.log('âœ… Session updated');
              break;

            case 'conversation.item.created':
              console.log('ğŸ’¬ Conversation item:', data.item);
              if (data.item && data.item.type === 'message') {
                const content = data.item.content?.[0];
                const text = content?.transcript || content?.text || '';
                if (text) {
                  setState(prev => ({
                    ...prev,
                    history: [...prev.history, {
                      role: data.item.role,
                      content: text,
                      timestamp: new Date(),
                    }],
                  }));
                }
              }
              break;

            case 'input_audio_transcription.completed':
              console.log('ğŸ¤ User said:', data.transcript);
              if (data.transcript) {
                setState(prev => ({
                  ...prev,
                  history: [...prev.history, {
                    role: 'user',
                    content: data.transcript,
                    timestamp: new Date(),
                  }],
                }));
              }
              break;

            case 'response.audio.delta':
              console.log('ğŸ”Š Audio delta received');
              break;

            case 'response.audio.done':
              console.log('ğŸ”Š Audio response complete');
              break;

            case 'response.text.delta':
              // ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ«ã‚¿æ›´æ–°
              break;

            case 'response.text.done':
              console.log('ğŸ“ Text response:', data.text);
              if (data.text) {
                setState(prev => ({
                  ...prev,
                  history: [...prev.history, {
                    role: 'assistant',
                    content: data.text,
                    timestamp: new Date(),
                  }],
                }));
              }
              break;

            case 'response.audio_transcript.done':
              console.log('ğŸ“ Audio transcript:', data.transcript);
              if (data.transcript) {
                setState(prev => ({
                  ...prev,
                  history: [...prev.history, {
                    role: 'assistant',
                    content: data.transcript,
                    timestamp: new Date(),
                  }],
                }));
              }
              break;

            case 'error':
              console.error('âŒ Server error:', data.error);
              setState(prev => ({ 
                ...prev, 
                error: data.error?.message || 'Server error' 
              }));
              break;

            default:
              console.log('ğŸ“¥ Other event:', data.type, data);
          }
        } catch (error) {
          console.error('âŒ Failed to parse message:', error);
        }
      };

      dc.onerror = (error) => {
        console.error('âŒ Data channel error:', error);
      };

      // SDP offerä½œæˆ
      console.log('ğŸ“¤ Creating SDP offer...');
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      // ICEå€™è£œåé›†ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…ã¤ï¼ˆæœ€å¤§3ç§’ï¼‰
      console.log('â³ Waiting for ICE gathering...');
      await Promise.race([
        iceGatheringComplete,
        new Promise(resolve => setTimeout(resolve, 3000))
      ]);

      // æœ€çµ‚çš„ãªSDP
      const finalOffer = pc.localDescription;
      if (!finalOffer || !finalOffer.sdp) {
        throw new Error('Failed to create SDP offer');
      }

      console.log('ğŸ“‹ SDP offer ready, length:', finalOffer.sdp.length);

      // ãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãæŒ‡å®šã™ã‚‹æ–¹æ³•ã‚’è©¦ã™
      const model = 'gpt-4o-realtime-preview-2025-06-03';
      
      // æ–¹æ³•1: URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦è¿½åŠ 
      const url = new URL('https://api.openai.com/v1/realtime');
      url.searchParams.set('model', model);
      
      console.log('ğŸŒ Connecting to:', url.toString());
      console.log('ğŸ” Using API key starting with:', apiKey.substring(0, 20));
      
      const response = await fetch(url, {
        method: 'POST',
        body: finalOffer.sdp,
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/sdp',
        },
      });

      console.log('ğŸ“¨ Response status:', response.status);
      console.log('ğŸ“¨ Response headers:', Object.fromEntries(response.headers.entries()));

      if (!response.ok) {
        const errorText = await response.text();
        console.error('âŒ API Error:', errorText);
        
        // ã‚¨ãƒ©ãƒ¼ã®è©³ç´°è§£æ
        try {
          const errorData = JSON.parse(errorText);
          console.error('âŒ Parsed error:', errorData);
          
          if (errorData.error?.code === 'invalid_api_key') {
            throw new Error('APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚.env.localã®NEXT_PUBLIC_OPENAI_API_KEYã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚');
          }
        } catch (e) {
          // JSON parse error - ignore
        }
        
        throw new Error(`API Error: ${response.status} - ${errorText}`);
      }

      // SDP answerå—ä¿¡
      const answerSdp = await response.text();
      console.log('ğŸ“¥ Received SDP answer, length:', answerSdp.length);
      
      const answer = { 
        type: 'answer' as RTCSdpType, 
        sdp: answerSdp 
      };
      
      await pc.setRemoteDescription(answer);
      console.log('âœ… WebRTC connection established!');

    } catch (error) {
      console.error('âŒ Connection error:', error);
      setState(prev => ({ 
        ...prev, 
        isConnecting: false, 
        error: error instanceof Error ? error.message : 'Connection failed' 
      }));
      
      // Cleanup
      if (pcRef.current) {
        pcRef.current.close();
        pcRef.current = null;
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }
      if (audioRef.current) {
        audioRef.current.remove();
        audioRef.current = null;
      }
    }
  }, []);

  const disconnect = useCallback(() => {
    console.log('ğŸ”´ Disconnecting...');
    
    if (dcRef.current) {
      dcRef.current.close();
      dcRef.current = null;
    }
    if (pcRef.current) {
      pcRef.current.close();
      pcRef.current = null;
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (audioRef.current) {
      audioRef.current.srcObject = null;
      audioRef.current.remove();
      audioRef.current = null;
    }
    
    setState(prev => ({ 
      ...prev, 
      isConnected: false,
      scenario: null,
      history: [],
    }));
  }, []);

  const toggleMute = useCallback(() => {
    if (streamRef.current) {
      const audioTrack = streamRef.current.getAudioTracks()[0];
      if (audioTrack) {
        const newMuted = !state.isMuted;
        audioTrack.enabled = !newMuted;
        setState(prev => ({ ...prev, isMuted: newMuted }));
        console.log(newMuted ? 'ğŸ”‡ Muted' : 'ğŸ”Š Unmuted');
        
        // ãƒŸãƒ¥ãƒ¼ãƒˆçŠ¶æ…‹ã®å¤‰æ›´ã‚’é€šçŸ¥
        if (dcRef.current && dcRef.current.readyState === 'open') {
          if (newMuted) {
            // ãƒŸãƒ¥ãƒ¼ãƒˆæ™‚ã¯éŸ³å£°å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢
            dcRef.current.send(JSON.stringify({
              type: 'input_audio_buffer.clear'
            }));
          }
        }
      }
    }
  }, [state.isMuted]);

  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return { state, connect, disconnect, toggleMute };
}